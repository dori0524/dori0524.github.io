---
title:  "[ML] 1.2 Maximum Likelihood Estimation"
search: false
categories: Machine learning
  - Jekyll
last_modified_at: 2019-05-11T11:30:00-05:00
use_math: true
---

# 인공지능 및 기계학습 개론 I
문일철 교수님의 강의를 정리하는 페이지 입니다.

## 1.2 Maximum Likelihood Estimation

### Thumbtack Qeustion
  압정을 던져서 head가 나올 확률을 계산해보자


### Experience from trials
가장 쉬운 방법은 실제로 압정을 던져 head와 tail이 나올 확률을 계산한다.

그럼 이게 다인가? (좀 복잡하게 봐봅시다.)

### Binomial Distribution
Thumbtack question에서는 압정이 나타낼 수 있는 경우가 head와 tail 두가지만 존재하며 각각의 결과는 서로의 결과에 독립적이다.
이러한 특징을 가지는 문제를 이산확률분포 특히 두가지의 경우만 있는 경우 binomial distribution으로 표혆이 가능하다.

### Bernoulli experiment
앞에서 head가 나오는 확률을 계산하기 위해서 압정을 던지는 것을 Bernoulli experiment라고 한다.

실험은  i.i.d 를 만족한다고 가정을 하고 수행한다.
Independent events: 각 실험이 서로 연관된 실험이 아님
Identically distributed: 매 실험시 확률은 변하지 않음

** 수식으로 나타내 보자~!! **

압정의 Head가 나올 확률이 theta 일때,실험을 통해서 얻은 Data (D)가 나오 확률 $P(D|\theta)$
Data: $D$
Experimental trials: $n$
The number of heads: $a_H$
The number of tails: $a_T$
Probability of Head: $P(H)=\theta$
Probability of Tail: $P(T)=1-\theta$

$P(D|\theta) = \theta ^{a_H} (1-\theta) ^{a_T}$

**따라서, 5회 실험시 HHTHT가 나올 확률은**
Data ($D$) = $H,H,T,H,T$
Experimental trails ($n$) = 5
The number of heads ($a_H$) = 3
The number of tails ($a_T$) = 2
$ P(D|\theta) = \theta^3(1-\theta)^2$

### Maximum Likelihood Estimation
위 수식을 구하기 위해 사용된 가정은 아래와 같다.
** The gambiling result of thumbtack follows the binomial distribution of $\theta$**

고로 데이터가 binomial distribution을 따른 다는 가정하에 데이터를 가장 잘 설명할 수 있는 best candidate of theta를 찾는 것이 필요하다.

이것을 수행하기 위한 방법의 하나로 Maximum Likelihood Estimation (MLE)라는 방법에 대해서 알아보자.

MLE는 관측된 데이터가 발생활 확룰을 최대화 하는 theta를 찾는 방법이며 수식으로는 아래와 같이 표현한다.

$\hat{\theta}=argmax_\theta P(D|\theta)$

** MLE를 위 5회 수행한 실험 HHTHT에 적용해보자**
1) 먼저 $\hat{\theta}$에 자연로그($ln$)를 취하자

$ln\hat{\theta}=argmax_\theta[a_Hln\theta+a_Tln(1-\theta)]$

2) 최대값을 찾기 위해서 미분해서 0이 되는 지점을 찾자

${d \over d\theta}(a_Hln\theta+a_Tln(1-\theta))=0$

${a_H \over \theta}-{a_T \over 1-\theta}=0$

$\theta={a_H \over a_T + a_H}$

따라서, $\theta$ 가 ${a_H \over a_T + a_H}$와 같을 때 MLE측면에서 binomial distribution을 따르는 최적을 $\theta$가 된다.

$\hat{\theta} = {a_H \over a_H + a_T}$

### Number of Trials
위 실험은 5회 수행했지만 다른 실험을 통해 50회 수행했는데 동일한 $\theta$가 나왔다면 더 많은 실험을 했을때 가지는 의미는 무엇일까?

실험에서는 항상 error ($\varepsilon$)가 존재하며 실험의 횟수가 증가할수록 error가 줄어드는 효과가 있다.

** 이를 수식으로 확인해 보자.**

True parameter ($\theta$): $\hat{\theta}$
Estimated parameter ($\theta$): $\theta$
$P(| \hat{\theta}-\theta | \ge \varepsilon) \le 2e^{-2N\varepsilon^2}$

위 식의 의미는 true parameter와 estimated parameter의 차가 error보다 클 확률은 위 식의 우항보다 작다라는 의미이다.

따라서, 총 실험의 수 (N)이 커질수록 true parameter와 esimated parameter의 차이가 줄어들게 된다.

반대로 $\varepsilon=0.1$이 되는 N값도 찾을 수 있게된다.

이러한 이론을 근거로하여 학습을 하는 것을 probability approximation correct (PAC) learning이라고 한다.





